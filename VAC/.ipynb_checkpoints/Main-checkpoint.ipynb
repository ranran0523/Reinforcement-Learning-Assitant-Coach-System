{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ranra\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ranra\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ranra\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ranra\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ranra\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ranra\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "from DDPG import *\n",
    "from nba_env import *\n",
    "from conf import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 | Steps: 46 | Explore: 1.99 | Reward: 39.11 | Win : 19.00 \n",
      "Ep: 1 | Steps: 46 | Explore: 1.94 | Reward: 42.59 | Win : 24.00 \n",
      "Ep: 2 | Steps: 46 | Explore: 1.90 | Reward: 39.50 | Win : 22.00 \n",
      "Ep: 3 | Steps: 46 | Explore: 1.85 | Reward: 40.58 | Win : 24.00 \n",
      "Ep: 4 | Steps: 46 | Explore: 1.81 | Reward: 42.40 | Win : 27.00 \n",
      "Ep: 5 | Steps: 46 | Explore: 1.77 | Reward: 36.89 | Win : 22.00 \n",
      "Ep: 6 | Steps: 46 | Explore: 1.73 | Reward: 41.71 | Win : 27.00 \n",
      "Ep: 7 | Steps: 46 | Explore: 1.69 | Reward: 36.50 | Win : 22.00 \n",
      "Ep: 8 | Steps: 46 | Explore: 1.65 | Reward: 30.48 | Win : 16.00 \n",
      "Ep: 9 | Steps: 46 | Explore: 1.61 | Reward: 37.44 | Win : 23.00 \n",
      "Ep: 10 | Steps: 46 | Explore: 1.58 | Reward: 43.34 | Win : 29.00 \n",
      "Ep: 11 | Steps: 46 | Explore: 1.54 | Reward: 39.37 | Win : 25.00 \n",
      "Ep: 12 | Steps: 46 | Explore: 1.51 | Reward: 36.26 | Win : 22.00 \n",
      "Ep: 13 | Steps: 46 | Explore: 1.47 | Reward: 36.19 | Win : 22.00 \n",
      "Ep: 14 | Steps: 46 | Explore: 1.44 | Reward: 32.28 | Win : 18.00 \n",
      "Ep: 15 | Steps: 46 | Explore: 1.41 | Reward: 35.24 | Win : 21.00 \n",
      "Ep: 16 | Steps: 46 | Explore: 1.37 | Reward: 42.14 | Win : 28.00 \n",
      "Ep: 17 | Steps: 46 | Explore: 1.34 | Reward: 36.17 | Win : 22.00 \n",
      "Ep: 18 | Steps: 46 | Explore: 1.31 | Reward: 40.20 | Win : 26.00 \n",
      "Ep: 19 | Steps: 46 | Explore: 1.28 | Reward: 40.30 | Win : 26.00 \n",
      "Ep: 20 | Steps: 46 | Explore: 1.25 | Reward: 40.95 | Win : 27.00 \n",
      "Ep: 21 | Steps: 46 | Explore: 1.23 | Reward: 40.23 | Win : 27.00 \n",
      "Ep: 22 | Steps: 46 | Explore: 1.20 | Reward: 35.98 | Win : 23.00 \n",
      "Ep: 23 | Steps: 46 | Explore: 1.17 | Reward: 37.98 | Win : 25.00 \n",
      "Ep: 24 | Steps: 46 | Explore: 1.14 | Reward: 34.78 | Win : 22.00 \n",
      "Ep: 25 | Steps: 46 | Explore: 1.12 | Reward: 36.83 | Win : 24.00 \n",
      "Ep: 26 | Steps: 46 | Explore: 1.09 | Reward: 39.79 | Win : 27.00 \n",
      "Ep: 27 | Steps: 46 | Explore: 1.07 | Reward: 35.81 | Win : 23.00 \n",
      "Ep: 28 | Steps: 46 | Explore: 1.04 | Reward: 37.73 | Win : 25.00 \n",
      "Ep: 29 | Steps: 46 | Explore: 1.02 | Reward: 31.82 | Win : 19.00 \n",
      "Ep: 30 | Steps: 46 | Explore: 1.00 | Reward: 30.70 | Win : 18.00 \n",
      "Ep: 31 | Steps: 46 | Explore: 0.97 | Reward: 31.48 | Win : 19.00 \n",
      "Ep: 32 | Steps: 46 | Explore: 0.95 | Reward: 34.12 | Win : 22.00 \n",
      "Ep: 33 | Steps: 46 | Explore: 0.93 | Reward: 35.15 | Win : 23.00 \n",
      "Ep: 34 | Steps: 46 | Explore: 0.91 | Reward: 41.13 | Win : 29.00 \n",
      "Ep: 35 | Steps: 46 | Explore: 0.89 | Reward: 33.05 | Win : 21.00 \n",
      "Ep: 36 | Steps: 46 | Explore: 0.87 | Reward: 34.95 | Win : 23.00 \n",
      "Ep: 37 | Steps: 46 | Explore: 0.85 | Reward: 34.05 | Win : 22.00 \n",
      "Ep: 38 | Steps: 46 | Explore: 0.83 | Reward: 45.99 | Win : 34.00 \n",
      "Ep: 39 | Steps: 46 | Explore: 0.81 | Reward: 29.01 | Win : 17.00 \n",
      "Ep: 40 | Steps: 46 | Explore: 0.79 | Reward: 31.98 | Win : 20.00 \n",
      "Ep: 41 | Steps: 46 | Explore: 0.77 | Reward: 42.03 | Win : 30.00 \n",
      "Ep: 42 | Steps: 46 | Explore: 0.76 | Reward: 39.01 | Win : 27.00 \n",
      "Ep: 43 | Steps: 46 | Explore: 0.74 | Reward: 39.00 | Win : 27.00 \n",
      "Ep: 44 | Steps: 46 | Explore: 0.72 | Reward: 36.03 | Win : 24.00 \n",
      "Ep: 45 | Steps: 46 | Explore: 0.71 | Reward: 38.01 | Win : 26.00 \n",
      "Ep: 46 | Steps: 46 | Explore: 0.69 | Reward: 39.03 | Win : 27.00 \n",
      "Ep: 47 | Steps: 46 | Explore: 0.67 | Reward: 37.93 | Win : 26.00 \n",
      "Ep: 48 | Steps: 46 | Explore: 0.66 | Reward: 33.84 | Win : 22.00 \n",
      "Ep: 49 | Steps: 46 | Explore: 0.64 | Reward: 37.81 | Win : 26.00 \n",
      "Ep: 50 | Steps: 46 | Explore: 0.63 | Reward: 33.80 | Win : 22.00 \n",
      "Ep: 51 | Steps: 46 | Explore: 0.61 | Reward: 30.80 | Win : 19.00 \n",
      "Ep: 52 | Steps: 46 | Explore: 0.60 | Reward: 29.83 | Win : 18.00 \n",
      "Ep: 53 | Steps: 46 | Explore: 0.59 | Reward: 31.83 | Win : 20.00 \n",
      "Ep: 54 | Steps: 46 | Explore: 0.57 | Reward: 38.84 | Win : 27.00 \n",
      "Ep: 55 | Steps: 46 | Explore: 0.56 | Reward: 36.80 | Win : 25.00 \n",
      "Ep: 56 | Steps: 46 | Explore: 0.55 | Reward: 37.81 | Win : 26.00 \n",
      "Ep: 57 | Steps: 46 | Explore: 0.54 | Reward: 35.75 | Win : 24.00 \n",
      "Ep: 58 | Steps: 46 | Explore: 0.52 | Reward: 33.77 | Win : 22.00 \n",
      "Ep: 59 | Steps: 46 | Explore: 0.51 | Reward: 34.78 | Win : 23.00 \n",
      "Ep: 60 | Steps: 46 | Explore: 0.50 | Reward: 27.80 | Win : 16.00 \n",
      "Ep: 61 | Steps: 46 | Explore: 0.49 | Reward: 30.86 | Win : 19.00 \n",
      "Ep: 62 | Steps: 46 | Explore: 0.48 | Reward: 35.71 | Win : 24.00 \n",
      "Ep: 63 | Steps: 46 | Explore: 0.47 | Reward: 37.70 | Win : 26.00 \n",
      "Ep: 64 | Steps: 46 | Explore: 0.46 | Reward: 33.69 | Win : 22.00 \n",
      "Ep: 65 | Steps: 46 | Explore: 0.45 | Reward: 29.68 | Win : 18.00 \n",
      "Ep: 66 | Steps: 46 | Explore: 0.44 | Reward: 36.68 | Win : 25.00 \n",
      "Ep: 67 | Steps: 46 | Explore: 0.43 | Reward: 32.69 | Win : 21.00 \n",
      "Ep: 68 | Steps: 46 | Explore: 0.42 | Reward: 30.70 | Win : 19.00 \n",
      "Ep: 69 | Steps: 46 | Explore: 0.41 | Reward: 31.70 | Win : 20.00 \n",
      "Ep: 70 | Steps: 46 | Explore: 0.40 | Reward: 45.69 | Win : 34.00 \n",
      "Ep: 71 | Steps: 46 | Explore: 0.39 | Reward: 36.67 | Win : 25.00 \n",
      "Ep: 72 | Steps: 46 | Explore: 0.38 | Reward: 32.67 | Win : 21.00 \n",
      "Ep: 73 | Steps: 46 | Explore: 0.37 | Reward: 36.68 | Win : 25.00 \n",
      "Ep: 74 | Steps: 46 | Explore: 0.36 | Reward: 34.65 | Win : 23.00 \n",
      "Ep: 75 | Steps: 46 | Explore: 0.35 | Reward: 39.63 | Win : 28.00 \n",
      "Ep: 76 | Steps: 46 | Explore: 0.35 | Reward: 35.66 | Win : 24.00 \n",
      "Ep: 77 | Steps: 46 | Explore: 0.34 | Reward: 36.62 | Win : 25.00 \n",
      "Ep: 78 | Steps: 46 | Explore: 0.33 | Reward: 32.59 | Win : 21.00 \n",
      "Ep: 79 | Steps: 46 | Explore: 0.32 | Reward: 35.60 | Win : 24.00 \n",
      "Ep: 80 | Steps: 46 | Explore: 0.32 | Reward: 29.61 | Win : 18.00 \n",
      "Ep: 81 | Steps: 46 | Explore: 0.31 | Reward: 34.59 | Win : 23.00 \n",
      "Ep: 82 | Steps: 46 | Explore: 0.30 | Reward: 36.58 | Win : 25.00 \n",
      "Ep: 83 | Steps: 46 | Explore: 0.29 | Reward: 36.60 | Win : 25.00 \n",
      "Ep: 84 | Steps: 46 | Explore: 0.29 | Reward: 29.62 | Win : 18.00 \n",
      "Ep: 85 | Steps: 46 | Explore: 0.28 | Reward: 36.60 | Win : 25.00 \n",
      "Ep: 86 | Steps: 46 | Explore: 0.27 | Reward: 34.58 | Win : 23.00 \n",
      "Ep: 87 | Steps: 46 | Explore: 0.27 | Reward: 33.59 | Win : 22.00 \n",
      "Ep: 88 | Steps: 46 | Explore: 0.26 | Reward: 42.61 | Win : 31.00 \n",
      "Ep: 89 | Steps: 46 | Explore: 0.26 | Reward: 39.60 | Win : 28.00 \n",
      "Ep: 90 | Steps: 46 | Explore: 0.25 | Reward: 34.60 | Win : 23.00 \n",
      "Ep: 91 | Steps: 46 | Explore: 0.24 | Reward: 29.57 | Win : 18.00 \n",
      "Ep: 92 | Steps: 46 | Explore: 0.24 | Reward: 41.61 | Win : 30.00 \n",
      "Ep: 93 | Steps: 46 | Explore: 0.23 | Reward: 30.56 | Win : 19.00 \n",
      "Ep: 94 | Steps: 46 | Explore: 0.23 | Reward: 35.54 | Win : 24.00 \n",
      "Ep: 95 | Steps: 46 | Explore: 0.22 | Reward: 27.54 | Win : 16.00 \n",
      "Ep: 96 | Steps: 46 | Explore: 0.22 | Reward: 36.57 | Win : 25.00 \n",
      "Ep: 97 | Steps: 46 | Explore: 0.21 | Reward: 33.58 | Win : 22.00 \n",
      "Ep: 98 | Steps: 46 | Explore: 0.21 | Reward: 30.54 | Win : 19.00 \n",
      "Ep: 99 | Steps: 46 | Explore: 0.20 | Reward: 31.55 | Win : 20.00 \n",
      "Ep: 100 | Steps: 46 | Explore: 0.20 | Reward: 32.57 | Win : 21.00 \n",
      "Ep: 101 | Steps: 46 | Explore: 0.19 | Reward: 32.56 | Win : 21.00 \n",
      "Ep: 102 | Steps: 46 | Explore: 0.19 | Reward: 32.54 | Win : 21.00 \n",
      "Ep: 103 | Steps: 46 | Explore: 0.19 | Reward: 39.55 | Win : 28.00 \n",
      "Ep: 104 | Steps: 46 | Explore: 0.18 | Reward: 34.54 | Win : 23.00 \n",
      "Ep: 105 | Steps: 46 | Explore: 0.18 | Reward: 34.55 | Win : 23.00 \n",
      "Ep: 106 | Steps: 46 | Explore: 0.17 | Reward: 32.55 | Win : 21.00 \n",
      "Ep: 107 | Steps: 46 | Explore: 0.17 | Reward: 27.56 | Win : 16.00 \n",
      "Ep: 108 | Steps: 46 | Explore: 0.17 | Reward: 31.56 | Win : 20.00 \n",
      "Ep: 109 | Steps: 46 | Explore: 0.16 | Reward: 32.52 | Win : 21.00 \n",
      "Ep: 110 | Steps: 46 | Explore: 0.16 | Reward: 35.55 | Win : 24.00 \n",
      "Ep: 111 | Steps: 46 | Explore: 0.15 | Reward: 35.48 | Win : 24.00 \n",
      "Ep: 112 | Steps: 46 | Explore: 0.15 | Reward: 33.50 | Win : 22.00 \n",
      "Ep: 113 | Steps: 46 | Explore: 0.15 | Reward: 35.52 | Win : 24.00 \n",
      "Ep: 114 | Steps: 46 | Explore: 0.14 | Reward: 40.52 | Win : 29.00 \n",
      "Ep: 115 | Steps: 46 | Explore: 0.14 | Reward: 36.54 | Win : 25.00 \n",
      "Ep: 116 | Steps: 46 | Explore: 0.14 | Reward: 39.55 | Win : 28.00 \n",
      "Ep: 117 | Steps: 46 | Explore: 0.13 | Reward: 30.52 | Win : 19.00 \n",
      "Ep: 118 | Steps: 46 | Explore: 0.13 | Reward: 39.53 | Win : 28.00 \n",
      "Ep: 119 | Steps: 46 | Explore: 0.13 | Reward: 34.52 | Win : 23.00 \n",
      "Ep: 120 | Steps: 46 | Explore: 0.13 | Reward: 33.52 | Win : 22.00 \n",
      "Ep: 121 | Steps: 46 | Explore: 0.12 | Reward: 39.52 | Win : 28.00 \n",
      "Ep: 122 | Steps: 46 | Explore: 0.12 | Reward: 34.54 | Win : 23.00 \n",
      "Ep: 123 | Steps: 46 | Explore: 0.12 | Reward: 39.50 | Win : 28.00 \n",
      "Ep: 124 | Steps: 46 | Explore: 0.11 | Reward: 35.49 | Win : 24.00 \n",
      "Ep: 125 | Steps: 46 | Explore: 0.11 | Reward: 30.52 | Win : 19.00 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 126 | Steps: 46 | Explore: 0.11 | Reward: 34.51 | Win : 23.00 \n",
      "Ep: 127 | Steps: 46 | Explore: 0.11 | Reward: 37.51 | Win : 26.00 \n",
      "Ep: 128 | Steps: 46 | Explore: 0.10 | Reward: 30.50 | Win : 19.00 \n",
      "Ep: 129 | Steps: 46 | Explore: 0.10 | Reward: 33.55 | Win : 22.00 \n",
      "Ep: 130 | Steps: 46 | Explore: 0.10 | Reward: 36.56 | Win : 25.00 \n",
      "Ep: 131 | Steps: 46 | Explore: 0.10 | Reward: 32.51 | Win : 21.00 \n",
      "Ep: 132 | Steps: 46 | Explore: 0.10 | Reward: 33.53 | Win : 22.00 \n",
      "Ep: 133 | Steps: 46 | Explore: 0.10 | Reward: 32.57 | Win : 21.00 \n",
      "Ep: 134 | Steps: 46 | Explore: 0.10 | Reward: 34.55 | Win : 23.00 \n",
      "Ep: 135 | Steps: 46 | Explore: 0.10 | Reward: 39.56 | Win : 28.00 \n",
      "Ep: 136 | Steps: 46 | Explore: 0.10 | Reward: 32.54 | Win : 21.00 \n",
      "Ep: 137 | Steps: 46 | Explore: 0.10 | Reward: 38.58 | Win : 27.00 \n",
      "Ep: 138 | Steps: 46 | Explore: 0.10 | Reward: 36.57 | Win : 25.00 \n",
      "Ep: 139 | Steps: 46 | Explore: 0.10 | Reward: 32.55 | Win : 21.00 \n",
      "Ep: 140 | Steps: 46 | Explore: 0.10 | Reward: 33.56 | Win : 22.00 \n",
      "Ep: 141 | Steps: 46 | Explore: 0.10 | Reward: 37.55 | Win : 26.00 \n",
      "Ep: 142 | Steps: 46 | Explore: 0.10 | Reward: 27.55 | Win : 16.00 \n",
      "Ep: 143 | Steps: 46 | Explore: 0.10 | Reward: 32.56 | Win : 21.00 \n",
      "Ep: 144 | Steps: 46 | Explore: 0.10 | Reward: 41.58 | Win : 30.00 \n",
      "Ep: 145 | Steps: 46 | Explore: 0.10 | Reward: 30.55 | Win : 19.00 \n",
      "Ep: 146 | Steps: 46 | Explore: 0.10 | Reward: 34.57 | Win : 23.00 \n",
      "Ep: 147 | Steps: 46 | Explore: 0.10 | Reward: 33.58 | Win : 22.00 \n",
      "Ep: 148 | Steps: 46 | Explore: 0.10 | Reward: 34.58 | Win : 23.00 \n",
      "Ep: 149 | Steps: 46 | Explore: 0.10 | Reward: 25.58 | Win : 14.00 \n",
      "Ep: 150 | Steps: 46 | Explore: 0.10 | Reward: 30.58 | Win : 19.00 \n",
      "Ep: 151 | Steps: 46 | Explore: 0.10 | Reward: 35.61 | Win : 24.00 \n",
      "Ep: 152 | Steps: 46 | Explore: 0.10 | Reward: 34.57 | Win : 23.00 \n",
      "Ep: 153 | Steps: 46 | Explore: 0.10 | Reward: 34.58 | Win : 23.00 \n",
      "Ep: 154 | Steps: 46 | Explore: 0.10 | Reward: 35.60 | Win : 24.00 \n",
      "Ep: 155 | Steps: 46 | Explore: 0.10 | Reward: 32.61 | Win : 21.00 \n",
      "Ep: 156 | Steps: 46 | Explore: 0.10 | Reward: 32.60 | Win : 21.00 \n",
      "Ep: 157 | Steps: 46 | Explore: 0.10 | Reward: 34.60 | Win : 23.00 \n",
      "Ep: 158 | Steps: 46 | Explore: 0.10 | Reward: 32.58 | Win : 21.00 \n",
      "Ep: 159 | Steps: 46 | Explore: 0.10 | Reward: 38.61 | Win : 27.00 \n",
      "Ep: 160 | Steps: 46 | Explore: 0.10 | Reward: 38.56 | Win : 27.00 \n",
      "Ep: 161 | Steps: 46 | Explore: 0.10 | Reward: 35.59 | Win : 24.00 \n",
      "Ep: 162 | Steps: 46 | Explore: 0.10 | Reward: 37.60 | Win : 26.00 \n",
      "Ep: 163 | Steps: 46 | Explore: 0.10 | Reward: 32.62 | Win : 21.00 \n",
      "Ep: 164 | Steps: 46 | Explore: 0.10 | Reward: 36.60 | Win : 25.00 \n",
      "Ep: 165 | Steps: 46 | Explore: 0.10 | Reward: 33.63 | Win : 22.00 \n",
      "Ep: 166 | Steps: 46 | Explore: 0.10 | Reward: 37.61 | Win : 26.00 \n",
      "Ep: 167 | Steps: 46 | Explore: 0.10 | Reward: 32.62 | Win : 21.00 \n",
      "Ep: 168 | Steps: 46 | Explore: 0.10 | Reward: 33.61 | Win : 22.00 \n",
      "Ep: 169 | Steps: 46 | Explore: 0.10 | Reward: 34.61 | Win : 23.00 \n",
      "Ep: 170 | Steps: 46 | Explore: 0.10 | Reward: 31.61 | Win : 20.00 \n",
      "Ep: 171 | Steps: 46 | Explore: 0.10 | Reward: 35.59 | Win : 24.00 \n",
      "Ep: 172 | Steps: 46 | Explore: 0.10 | Reward: 35.59 | Win : 24.00 \n",
      "Ep: 173 | Steps: 46 | Explore: 0.10 | Reward: 33.59 | Win : 22.00 \n",
      "Ep: 174 | Steps: 46 | Explore: 0.10 | Reward: 38.60 | Win : 27.00 \n",
      "Ep: 175 | Steps: 46 | Explore: 0.10 | Reward: 35.58 | Win : 24.00 \n",
      "Ep: 176 | Steps: 46 | Explore: 0.10 | Reward: 32.60 | Win : 21.00 \n",
      "Ep: 177 | Steps: 46 | Explore: 0.10 | Reward: 36.62 | Win : 25.00 \n",
      "Ep: 178 | Steps: 46 | Explore: 0.10 | Reward: 34.62 | Win : 23.00 \n",
      "Ep: 179 | Steps: 46 | Explore: 0.10 | Reward: 36.61 | Win : 25.00 \n",
      "Ep: 180 | Steps: 46 | Explore: 0.10 | Reward: 34.63 | Win : 23.00 \n",
      "Ep: 181 | Steps: 46 | Explore: 0.10 | Reward: 27.62 | Win : 16.00 \n",
      "Ep: 182 | Steps: 46 | Explore: 0.10 | Reward: 32.62 | Win : 21.00 \n",
      "Ep: 183 | Steps: 46 | Explore: 0.10 | Reward: 30.61 | Win : 19.00 \n",
      "Ep: 184 | Steps: 46 | Explore: 0.10 | Reward: 35.64 | Win : 24.00 \n",
      "Ep: 185 | Steps: 46 | Explore: 0.10 | Reward: 33.64 | Win : 22.00 \n",
      "Ep: 186 | Steps: 46 | Explore: 0.10 | Reward: 33.62 | Win : 22.00 \n",
      "Ep: 187 | Steps: 46 | Explore: 0.10 | Reward: 35.59 | Win : 24.00 \n",
      "Ep: 188 | Steps: 46 | Explore: 0.10 | Reward: 32.60 | Win : 21.00 \n",
      "Ep: 189 | Steps: 46 | Explore: 0.10 | Reward: 34.59 | Win : 23.00 \n",
      "Ep: 190 | Steps: 46 | Explore: 0.10 | Reward: 33.61 | Win : 22.00 \n",
      "Ep: 191 | Steps: 46 | Explore: 0.10 | Reward: 35.61 | Win : 24.00 \n",
      "Ep: 192 | Steps: 46 | Explore: 0.10 | Reward: 30.57 | Win : 19.00 \n",
      "Ep: 193 | Steps: 46 | Explore: 0.10 | Reward: 33.64 | Win : 22.00 \n",
      "Ep: 194 | Steps: 46 | Explore: 0.10 | Reward: 41.62 | Win : 30.00 \n",
      "Ep: 195 | Steps: 46 | Explore: 0.10 | Reward: 35.58 | Win : 24.00 \n",
      "Ep: 196 | Steps: 46 | Explore: 0.10 | Reward: 38.59 | Win : 27.00 \n",
      "Ep: 197 | Steps: 46 | Explore: 0.10 | Reward: 32.63 | Win : 21.00 \n",
      "Ep: 198 | Steps: 46 | Explore: 0.10 | Reward: 29.65 | Win : 18.00 \n",
      "Ep: 199 | Steps: 46 | Explore: 0.10 | Reward: 35.64 | Win : 24.00 \n",
      "\n",
      "Save Model ./save\\DDPG.ckpt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# 创建actor和critic网络\n",
    "actor = Actor(sess, ACTION_DIM, ACTION_BOUND[1], LR_A, REPLACE_ITER_A)\n",
    "\n",
    "critic = Critic(sess, STATE_DIM, ACTION_DIM, LR_C, GAMMA, REPLACE_ITER_C, actor.a, actor.a_)\n",
    "actor.add_grad_to_graph(critic.a_grads)\n",
    "\n",
    "# 创建记忆库\n",
    "M = Memory(MEMORY_CAPACITY, dims=2 * STATE_DIM + ACTION_DIM + 1)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "path = './save'\n",
    "\n",
    "# 如果LOAD为True则加载模型，否则初始化参数\n",
    "if LOAD:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "def train():\n",
    "    var = 2.                                                        # 探索度\n",
    "    reward = 0                                                      # 记录每次完整step的reward\n",
    "    win = 0 \n",
    "    for ep in range(MAX_EPISODES):\n",
    "        s = env.reset()                                             # 环境初始化，返回初始状态\n",
    "        ep_step = 0\n",
    "\n",
    "        for t in range(MAX_EP_STEPS):\n",
    "\n",
    "            # Added exploration noise\n",
    "            a = actor.choose_action(s)                              # 通过actor目标网络得到当前动作\n",
    "            a = np.clip(np.random.normal(a, var), *ACTION_BOUND)    # 给动作加上用于探索的随机量，并截断到0-48\n",
    "            s_, r, w,done = env.step(a)                               # 将动作作用到环境中，得到下一个状态和环境反馈的奖励，是否结果迭代\n",
    "            M.store_transition(s, a, r, s_)                         # 将s, a, r, s_保存到记忆库中\n",
    "            reward += r                                             # 加上本次step的奖励\n",
    "            win +=w\n",
    "\n",
    "            if M.pointer > MEMORY_CAPACITY:                         # 当记忆库的记录容量达到MEMORY_CAPACITY，开始训练\n",
    "                var = max([var * .9995, VAR_MIN])                   # 对动作的随机探索率进行衰减\n",
    "                b_M = M.sample(BATCH_SIZE)                          # 随机从记忆库中取出一个BATCH的数据进行训练\n",
    "                b_s = b_M[:, :STATE_DIM]\n",
    "                b_a = b_M[:, STATE_DIM: STATE_DIM + ACTION_DIM]\n",
    "                b_r = b_M[:, -STATE_DIM - 1: -STATE_DIM]\n",
    "                b_s_ = b_M[:, -STATE_DIM:]\n",
    "\n",
    "                critic.learn(b_s, b_a, b_r, b_s_)                   # 更新critic估计网络的参数\n",
    "                actor.learn(b_s)                                    # 更新actor估计网络的参数\n",
    "\n",
    "            s = s_                                                  # 将下一个状态作为下一次迭代的当前状态\n",
    "            ep_step += 1\n",
    "\n",
    "            if done or t == MAX_EP_STEPS - 1:                       # 当完整step结束后，即46场比赛结束后，打印信息\n",
    "                print('Ep:', ep,\n",
    "                      '| Steps: %i' % int(ep_step),\n",
    "                      '| Explore: %.2f' % var,\n",
    "                      '| Reward: %.2f' % reward,\n",
    "                      '| Win : %.2f '% win,\n",
    "                      )\n",
    "                reward = 0\n",
    "                win=0\n",
    "                break\n",
    "\n",
    "    if os.path.isdir(path): shutil.rmtree(path)                     # 保存参数\n",
    "    os.mkdir(path)\n",
    "    ckpt_path = os.path.join(path, 'DDPG.ckpt')\n",
    "    save_path = saver.save(sess, ckpt_path, write_meta_graph=False)\n",
    "    print(\"\\nSave Model %s\\n\" % save_path)\n",
    "\n",
    "\n",
    "def eval():\n",
    "    while True:\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            a = actor.choose_action(s)\n",
    "            s_, r, done = env.step(a)\n",
    "            s = s_\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if LOAD:\n",
    "        eval()\n",
    "    else:\n",
    "        train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
